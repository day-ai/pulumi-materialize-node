"use strict";
// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***
Object.defineProperty(exports, "__esModule", { value: true });
exports.SourceKafka = void 0;
const pulumi = require("@pulumi/pulumi");
const utilities = require("./utilities");
class SourceKafka extends pulumi.CustomResource {
    /**
     * Get an existing SourceKafka resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    static get(name, id, state, opts) {
        return new SourceKafka(name, state, { ...opts, id: id });
    }
    /**
     * Returns true if the given object is an instance of SourceKafka.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    static isInstance(obj) {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === SourceKafka.__pulumiType;
    }
    constructor(name, argsOrState, opts) {
        let resourceInputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState;
            resourceInputs["clusterName"] = state?.clusterName;
            resourceInputs["comment"] = state?.comment;
            resourceInputs["databaseName"] = state?.databaseName;
            resourceInputs["envelope"] = state?.envelope;
            resourceInputs["exposeProgress"] = state?.exposeProgress;
            resourceInputs["format"] = state?.format;
            resourceInputs["includeHeaders"] = state?.includeHeaders;
            resourceInputs["includeHeadersAlias"] = state?.includeHeadersAlias;
            resourceInputs["includeKey"] = state?.includeKey;
            resourceInputs["includeKeyAlias"] = state?.includeKeyAlias;
            resourceInputs["includeOffset"] = state?.includeOffset;
            resourceInputs["includeOffsetAlias"] = state?.includeOffsetAlias;
            resourceInputs["includePartition"] = state?.includePartition;
            resourceInputs["includePartitionAlias"] = state?.includePartitionAlias;
            resourceInputs["includeTimestamp"] = state?.includeTimestamp;
            resourceInputs["includeTimestampAlias"] = state?.includeTimestampAlias;
            resourceInputs["kafkaConnection"] = state?.kafkaConnection;
            resourceInputs["keyFormat"] = state?.keyFormat;
            resourceInputs["name"] = state?.name;
            resourceInputs["ownershipRole"] = state?.ownershipRole;
            resourceInputs["qualifiedSqlName"] = state?.qualifiedSqlName;
            resourceInputs["region"] = state?.region;
            resourceInputs["schemaName"] = state?.schemaName;
            resourceInputs["size"] = state?.size;
            resourceInputs["sourceKafkaId"] = state?.sourceKafkaId;
            resourceInputs["startOffsets"] = state?.startOffsets;
            resourceInputs["startTimestamp"] = state?.startTimestamp;
            resourceInputs["topic"] = state?.topic;
            resourceInputs["valueFormat"] = state?.valueFormat;
        }
        else {
            const args = argsOrState;
            if (args?.kafkaConnection === undefined && !opts.urn) {
                throw new Error("Missing required property 'kafkaConnection'");
            }
            if (args?.topic === undefined && !opts.urn) {
                throw new Error("Missing required property 'topic'");
            }
            resourceInputs["clusterName"] = args?.clusterName;
            resourceInputs["comment"] = args?.comment;
            resourceInputs["databaseName"] = args?.databaseName;
            resourceInputs["envelope"] = args?.envelope;
            resourceInputs["exposeProgress"] = args?.exposeProgress;
            resourceInputs["format"] = args?.format;
            resourceInputs["includeHeaders"] = args?.includeHeaders;
            resourceInputs["includeHeadersAlias"] = args?.includeHeadersAlias;
            resourceInputs["includeKey"] = args?.includeKey;
            resourceInputs["includeKeyAlias"] = args?.includeKeyAlias;
            resourceInputs["includeOffset"] = args?.includeOffset;
            resourceInputs["includeOffsetAlias"] = args?.includeOffsetAlias;
            resourceInputs["includePartition"] = args?.includePartition;
            resourceInputs["includePartitionAlias"] = args?.includePartitionAlias;
            resourceInputs["includeTimestamp"] = args?.includeTimestamp;
            resourceInputs["includeTimestampAlias"] = args?.includeTimestampAlias;
            resourceInputs["kafkaConnection"] = args?.kafkaConnection;
            resourceInputs["keyFormat"] = args?.keyFormat;
            resourceInputs["name"] = args?.name;
            resourceInputs["ownershipRole"] = args?.ownershipRole;
            resourceInputs["region"] = args?.region;
            resourceInputs["schemaName"] = args?.schemaName;
            resourceInputs["sourceKafkaId"] = args?.sourceKafkaId;
            resourceInputs["startOffsets"] = args?.startOffsets;
            resourceInputs["startTimestamp"] = args?.startTimestamp;
            resourceInputs["topic"] = args?.topic;
            resourceInputs["valueFormat"] = args?.valueFormat;
            resourceInputs["qualifiedSqlName"] = undefined /*out*/;
            resourceInputs["size"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(SourceKafka.__pulumiType, name, resourceInputs, opts, false /*dependency*/, utilities.getPackage());
    }
}
exports.SourceKafka = SourceKafka;
/** @internal */
SourceKafka.__pulumiType = 'materialize:index/sourceKafka:SourceKafka';
//# sourceMappingURL=sourceKafka.js.map